{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "サポートベクトルマシン",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1eqGq4jKa8uheldkuisSnm55lUzA7A9qB",
      "authorship_tag": "ABX9TyOjXKtlZSwBp7K7GVqcwyGo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gitboku/statistics-practice/blob/master/%E3%82%B5%E3%83%9D%E3%83%BC%E3%83%88%E3%83%99%E3%82%AF%E3%83%88%E3%83%AB%E3%83%9E%E3%82%B7%E3%83%B3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skh110fEizUD",
        "colab_type": "text"
      },
      "source": [
        "# サポートベクトルマシンとは"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVez2KtVnpnU",
        "colab_type": "text"
      },
      "source": [
        "参考：　https://logics-of-blue.com/svm-concept/\n",
        "\n",
        "外れ値のようなデータまで使うと予測精度が下がるので、本当に必要となるデータのみを使用する。\n",
        "この「本当に必要なデータ」をサポートベクトルと呼び、これを用いた機械学習がサポートベクトルマシン(Sapport vector machine: SVM)。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuLsA3iIoDPS",
        "colab_type": "text"
      },
      "source": [
        "## サポートベクトルとは"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kinkdf1RoE3k",
        "colab_type": "text"
      },
      "source": [
        "「マージン最大化」という考え方によって「予測に必要なデータ」を決定する。\n",
        "マージンとは判別する境界線とデータとの距離。\n",
        "マージンが大きい場合「ほんの少しデータが変わっただけで誤判定する」ということがなくなる。\n",
        "境界線と最も近くにあるデータをサポートベクトルと呼ぶ。\n",
        "\n",
        "境界線とサポートベクトルとのマージンを大きくすることで誤判定を防ぐ。\n",
        "\n",
        "\n",
        "はっきりとクラス分けできることを前提としたマージンをハードマージンと呼ぶ。\n",
        "一方、過学習を防ぐためにあえて誤分類をある程度許すようにしたマージンをソフトマージンと呼ぶ。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtosTnkreVri",
        "colab_type": "text"
      },
      "source": [
        "## ハードマージン\n",
        "\n",
        "参考： https://kenyu-life.com/2019/02/11/support_vector_machine/\n",
        "\n",
        "境界線の式を以下のように定義する。\n",
        "$w$は重み(n次元ベクトル)で、bはバイアス(スカラー)。\n",
        "$$\n",
        "w^Tx + b = 0\n",
        "$$\n",
        "\n",
        "ここで、$w^Tx + b > 0$となるxの集合を$K_1$とし、$w^Tx + b < 0$となるxの集合を$K_2$とする。\n",
        "\n",
        "さらに、$K_2$のラベルを-1ということにして、ラベルのベクトルを$t$で表せば、以下のように定義できる。\n",
        "\n",
        "$$\n",
        "t_i(w^Tx_i + b) > 0\\ (i = 1,2,\\cdots,n)\n",
        "$$\n",
        "\n",
        "### 境界線とのマージン\n",
        "\n",
        "2次元でのマージンは点と直線の距離の公式をそのまま使える。\n",
        "\n",
        "$$\n",
        "d = \\frac {|w_1x_1 + w_2x_2 + b|}{\\sqrt{w^2_1 + w^2_2}}\n",
        "$$\n",
        "\n",
        "これをn次元用に一般化すると以下のようになる(w, xはn次元ベクトル)。\n",
        "\n",
        "$$\n",
        "d = \\frac {|w^Tx + b|}{||w||}\n",
        "$$\n",
        "\n",
        "ラベルを使用した式に入れ替えると以下のようになる。\n",
        "\n",
        "$$\n",
        "\\frac {t_i(w^Tx_i + b)}{||w||} \\ge M\n",
        "$$\n",
        "\n",
        "「この式がすべてのクラスのサポートベクトルに対してマージン最大化する」と「すべてのベクトルが祖のマージンよりも距離が長い」という二つの条件を満たすことで、最適化する。\n",
        "\n",
        "ここで、両辺をMで割る。\n",
        "\n",
        "$$\n",
        "\\frac {t_i(w^Tx_i + b)}{M||w||} \\ge 1\n",
        "$$\n",
        "\n",
        "さらに、以下のように入れ替える。\n",
        "\n",
        "$$\n",
        "\\begin{eqnarray}\n",
        "\\tilde{w} &=& \\frac {w}{M||w||}\\\\\n",
        "\\tilde{b} &=& \\frac {b}{M||w||}\n",
        "\\end{eqnarray}\n",
        "$$\n",
        "\n",
        "最終的に以下のようになる。\n",
        "$$\n",
        "t_i(\\tilde{w}^Tx_i + \\tilde{b}) = 1\n",
        "$$\n",
        "\n",
        "サポートベクトルに対しては以下の統合が成立するので、これを目的関数とする。\n",
        "\n",
        "$$\n",
        "\\tilde{M} = \\frac {t_i(\\tilde{w}^Tx_i + \\tilde{b})}{||\\tilde{w}||} = \\frac {1}{||\\tilde{w}||}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ogr32NaLTTPd",
        "colab_type": "text"
      },
      "source": [
        "この「マージンを最大化する」という目標は「マージンの逆数を最小化する」というように書き換えられる。\n",
        "よって、以下のように変換する。\n",
        "二乗や2で割っているのは計算の簡単のため。\n",
        "\n",
        "$$\n",
        "\\min_{w,b}{\\frac {1}{2}} ||w||^2,\\ t_i(w^Tx_i + b) \\ge 1\\ (i=1,2,\\cdots,n)\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjNz55VleLkU",
        "colab_type": "text"
      },
      "source": [
        "## ソフトマージン\n",
        "\n",
        "ソフトマージンはハードマージンよりも懸命なSVM。\n",
        "線形分離不可能なデータだと、マージン内部にサポーベクトル以外のデータが入り込んでいる。\n",
        "\n",
        "そこで、ハードマージンの式にスラッグ変数$\\xi$を導入して制約を弱める。\n",
        "\n",
        "$$\n",
        "t_i(w^Tx_i + b) \\ge 1 - \\xi\\\\\n",
        "\\xi_i = max{0, M - \\frac {t_i(w^Tx_i + b)}{||w||}}\n",
        "$$\n",
        "\n",
        "例えば、マージンMが1の時、直線からの距離0.4の位置に別のデータが入り込んでいたとする。\n",
        "この場合、以下のように制約が小さくなる。\n",
        "\n",
        "$$\n",
        "\\begin{eqnarray}\n",
        "\\xi_i &=& \\max\\{0, M - \\frac {t_i(w^Tx_i + b)}{||w||}\\}\\\\\n",
        "\\xi_i &=& \\max\\{0, (1-0.4)\\}\\\\\n",
        "\\xi_i &=& 0.6\n",
        "\\end{eqnarray}\n",
        "$$\n",
        "\n",
        "ソフトマージンの目的関数は以下のようになる。\n",
        "\n",
        "$$\n",
        "\\min\\{\\frac{1}{2}||w^T||^2 + C\\sum_{i=1}^n\\xi_i\\}\n",
        "$$\n",
        "\n",
        "制約条件は以下の通りである。\n",
        "\n",
        "$$\n",
        "t_i(w^Tx_i + b) \\ge 1 - \\xi_i, \\xi_i \\ge 0\\ (i=1,2,\\cdots,n)\n",
        "$$\n",
        "\n",
        "ここで、Cは正規化係数であり、ペナルティの度合いでもある。\n",
        "この値が大きくなるほど$\\xi$を抑制する力が強くなり、$\\xi = \\infty$ではハードマージンと同じになる。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qv1vYe5vopyl",
        "colab_type": "text"
      },
      "source": [
        "## 線形から非線形SVMへの拡張\n",
        "\n",
        "双対問題を使用することによって、制約条件のない最適化問題への変形をする。\n",
        "\n",
        "ラグランジュの未定乗数法を使用する。\n",
        "\n",
        "$$\n",
        "L(x,a) = f(x) + \\sum_{i=1}^n a_ig_i(x)\n",
        "$$\n",
        "\n",
        "この時、制約条件は$t_i(w^Tx_i + b) \\ge 1 - \\xi_i$と$\\xi_i \\ge 0$の二つあることに注意して、ソフトマージン目的関数と制約条件をラグランジュ関数に当てはめる。\n",
        "\n",
        "$$\n",
        "L(w,b,\\xi,\\alpha,\\beta) =\\\\ \\frac{1}{2}||w||^2 + C\\sum_{i=1}^n\\xi_i - \\sum_{i=1}^n\\alpha_i\\{t_i(w^Tx_i + b) - 1 + \\xi_i\\} - \\sum_{i=1}^n\\beta_i\\xi_i\n",
        "$$\n",
        "\n",
        "これを主変数$(w,b,\\xi)$についてそれぞれ偏微分すると、以下のような式がそれぞれ得られる。\n",
        "\n",
        "$$\n",
        "\\begin{eqnarray}\n",
        "\\frac{\\partial L}{\\partial w} &=& w - \\sum_{i=1}^n \\alpha_it_ix_i = 0 &\\rightarrow& w = \\sum_{i=1}^n\\alpha_it_ix_i\\\\\n",
        "\\frac{\\partial L}{\\partial b} &=& - \\sum_{i=1}^n\\alpha_it_i = 0 &\\rightarrow& \\sum_{i=1}^n\\alpha_it_i = 0\\\\\n",
        "\\frac{\\partial L}{\\partial \\xi} &=& C - \\alpha_i - \\beta_i = 0 &\\rightarrow& C = \\alpha_i + \\beta_i\n",
        "\\end{eqnarray}\n",
        "$$\n",
        "\n",
        "この3つの式をラグランジュ関数に代入すると、以下の式に変形できる。\n",
        "\n",
        "$$\n",
        "\\max \\{\\tilde{L}(\\alpha) = \\sum_{i=1}^n\\alpha_i - \\frac{1}{2}\\sum_{i=1}^n\\sum_{i=j}^n\\alpha_i\\alpha_jt_it_jx_i^Tx_j\\}\n",
        "$$\n",
        "\n",
        "ここで$b$だけは主問題の制約を利用して導出する。\n",
        "\n",
        "$$\n",
        "b = \\frac{\\max_{t_i=-1}(wx_i) + \\min_{t_i=1}(wx_i)}{2}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9W6Ajk0RuYg8",
        "colab_type": "text"
      },
      "source": [
        "## カーネルの導入\n",
        "\n",
        "例えば、2次元空間では線形分離できないデータでも、3次元に拡張すれば線形分離ができるようになる。\n",
        "前節の最後のラグランジュ関数に代入した目的関数の後ろの$x_i^Tx_j$を$\\phi(x)_i^T\\phi(x)_j$に変形すると、以下のようになる。\n",
        "\n",
        "$$\n",
        "\\max \\{\\tilde{L}(\\alpha) = \\sum_{i=1}^n\\alpha_i - \\frac{1}{2}\\sum_{i=1}^n\\sum_{i=j}^n\\alpha_i\\alpha_jt_it_j\\phi(x)_i^T\\phi(x)_j\\}\n",
        "$$\n",
        "\n",
        "ここで$\\phi(x)$は$\\phi(x) = (\\phi_1(x),\\phi_2(x),\\cdots,\\phi_r(x))$である。\n",
        "\n",
        "しかし、特徴空間の次元数rは元の次元数nよりも非常に大きいため、内積の計算が難しい。\n",
        "よって、この特徴ベクトル$\\phi(x)_i^T\\phi(x)_j$の内積をカーネル関数で置き換える。\n",
        "\n",
        "ここで、カーネル関数には以下の例がある。\n",
        "\n",
        "・多項式カーネル\n",
        "$$\n",
        "K(x_i,x_j) = (x^T_ix_j)^d\n",
        "$$\n",
        "・ガウスカーネル\n",
        "$$\n",
        "K(x_i,x_j) = \\exp\\{{-\\frac{||x_i - x_j||^2}{2\\sigma^2}}\\}\n",
        "$$\n",
        "・シグモイドカーネル\n",
        "$$\n",
        "K(x_i,x_j) = tanh(bx^T_ix_j + c)\n",
        "$$\n",
        "\n",
        "上記の$c, d, \\sigma$は別途人間が決定する必要があるハイパーパラメータである。\n",
        "例えば、$\\sigma$は別のクラスの2点間の距離の最も小さいものを選択する。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QyE27m2lei1_",
        "colab_type": "text"
      },
      "source": [
        "# 実装"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZ3JKMm9elPe",
        "colab_type": "text"
      },
      "source": [
        "## データ用意\n",
        "\n",
        "以下のkaggleのページにあるサッカーのデータを利用\\\\\n",
        "https://www.kaggle.com/hugomathien/soccer\n",
        "\n",
        "sqliteファイルだったので、これをgoogleDriveの同じディレクトリに配置して読み込む。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8mn0DaSg4nT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sqlite3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LeWMWZrjgKwB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_file = open('/content/drive/My Drive/statistic/database.sqlite')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0uS89hjhEB8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "conn = sqlite3.connect('/content/drive/My Drive/statistic/soccerdata.db')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "be6Z9_hshTya",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}