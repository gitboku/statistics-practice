{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "サポーベクトルマシン",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOQ+YkgNXc2cvz1iiKOOlIc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gitboku/statistics-practice/blob/master/%E3%82%B5%E3%83%9D%E3%83%BC%E3%83%99%E3%82%AF%E3%83%88%E3%83%AB%E3%83%9E%E3%82%B7%E3%83%B3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skh110fEizUD",
        "colab_type": "text"
      },
      "source": [
        "# サポートベクトルマシンとは"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVez2KtVnpnU",
        "colab_type": "text"
      },
      "source": [
        "参考：　https://logics-of-blue.com/svm-concept/\n",
        "\n",
        "外れ値のようなデータまで使うと予測精度が下がるので、本当に必要となるデータのみを使用する。\n",
        "この「本当に必要なデータ」をサポートベクトルと呼び、これを用いた機械学習がサポートベクトルマシン(Sapport vector machine: SVM)。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuLsA3iIoDPS",
        "colab_type": "text"
      },
      "source": [
        "## サポートベクトルとは"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kinkdf1RoE3k",
        "colab_type": "text"
      },
      "source": [
        "「マージン最大化」という考え方によって「予測に必要なデータ」を決定する。\n",
        "マージンとは判別する境界線とデータとの距離。\n",
        "マージンが大きい場合「ほんの少しデータが変わっただけで誤判定する」ということがなくなる。\n",
        "境界線と最も近くにあるデータをサポートベクトルと呼ぶ。\n",
        "\n",
        "境界線とサポートベクトルとのマージンを大きくすることで誤判定を防ぐ。\n",
        "\n",
        "\n",
        "はっきりとクラス分けできることを前提としたマージンをハードマージンと呼ぶ。\n",
        "一方、過学習を防ぐためにあえて誤分類をある程度許すようにしたマージンをソフトマージンと呼ぶ。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtosTnkreVri",
        "colab_type": "text"
      },
      "source": [
        "## ハードマージン\n",
        "\n",
        "境界線の式を以下のように定義する。\n",
        "$w$は重み(n次元ベクトル)で、bはバイアス(スカラー)。\n",
        "$$\n",
        "w^Tx + b = 0\n",
        "$$\n",
        "\n",
        "ここで、$w^Tx + b > 0$となるxの集合を$K_1$とし、$w^Tx + b < 0$となるxの集合を$K_2$とする。\n",
        "\n",
        "さらに、$K_2$のラベルを-1ということにして、ラベルのベクトルを$t$で表せば、以下のように定義できる。\n",
        "\n",
        "$$\n",
        "t_i(w^Tx_i + b) > 0\\ (i = 1,2,\\cdots,n)\n",
        "$$\n",
        "\n",
        "### 境界線とのマージン\n",
        "\n",
        "2次元でのマージンは点と直線の距離の公式をそのまま使える。\n",
        "\n",
        "$$\n",
        "d = \\frac {|w_1x_1 + w_2x_2 + b|}{\\sqrt{w^2_1 + w^2_2}}\n",
        "$$\n",
        "\n",
        "これをn次元用に一般化すると以下のようになる(w, xはn次元ベクトル)。\n",
        "\n",
        "$$\n",
        "d = \\frac {|w^Tx + b|}{||w||}\n",
        "$$\n",
        "\n",
        "ラベルを使用した式に入れ替えると以下のようになる。\n",
        "\n",
        "$$\n",
        "\\frac {t_i(w^Tx_i + b)}{||w||} \\ge M\n",
        "$$\n",
        "\n",
        "「この式がすべてのクラスのサポートベクトルに対してマージン最大化する」と「すべてのベクトルが祖のマージンよりも距離が長い」という二つの条件を満たすことで、最適化する。\n",
        "\n",
        "ここで、両辺をMで割る。\n",
        "\n",
        "$$\n",
        "\\frac {t_i(w^Tx_i + b)}{M||w||} \\ge 1\n",
        "$$\n",
        "\n",
        "さらに、以下のように入れ替える。\n",
        "\n",
        "$$\n",
        "\\begin{eqnarray}\n",
        "\\tilde{w} &=& \\frac {w}{M||w||}\\\\\n",
        "\\tilde{b} &=& \\frac {b}{M||w||}\n",
        "\\end{eqnarray}\n",
        "$$\n",
        "\n",
        "最終的に以下のようになる。\n",
        "$$\n",
        "t_i(\\tilde{w}^Tx_i + \\tilde{b}) = 1\n",
        "$$\n",
        "\n",
        "サポートベクトルに対しては以下の統合が成立するので、これを目的関数とする。\n",
        "\n",
        "$$\n",
        "\\tilde{M} = \\frac {t_i(\\tilde{w}^Tx_i + \\tilde{b})}{||\\tilde{w}||} = \\frac {1}{||\\tilde{w}||}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjNz55VleLkU",
        "colab_type": "text"
      },
      "source": [
        "## ソフトマージン\n",
        "\n",
        "ソフトマージンは以下の値を最小にするようにする。\n",
        "ただし、$M$はマージンで$N$は誤判定数。\n",
        "$C$は「誤判定をどこまで許容するか」を示すパラメータで、大きければ許さないし、小さければあまり気にしない。\n",
        "$C$は人間が決めておく必要のあるハイパーパラメータであるが、グリッドサーチなどのようにチューニングする技術もある。\n",
        "\n",
        "$$\n",
        "\\min\\{ \\frac {1}{M} + C * N \\}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IyDLE4vz2g6C",
        "colab_type": "text"
      },
      "source": [
        "SVMは線形で分類するが、非線形のデータに対してはデータを変換してから分析するのがSVMで使われている手法。\n",
        "この変換には「ガウシアンカーネル」もしくは「多項式カーネル」という方法がよくつかわれる。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58QMy0-7itWH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}