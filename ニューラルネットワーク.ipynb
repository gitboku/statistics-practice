{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ニューラルネットワーク",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOwChYue2GgolRa/eVAL+wp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gitboku/statistics-practice/blob/master/%E3%83%8B%E3%83%A5%E3%83%BC%E3%83%A9%E3%83%AB%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0M3GKirajFHN",
        "colab_type": "text"
      },
      "source": [
        "# ニューラルネットワークとは\n",
        "\n",
        "パーセプトロンを大量にくっつけたもの。\n",
        "\n",
        "（すでにある程度の知識があるのでこれ以降は省略）\n",
        "\n",
        "\n",
        "## パーセプトロン\n",
        "\n",
        "式で表すと以下のようになる。\n",
        "\n",
        "$$\n",
        "\\begin{eqnarray}\n",
        "y &=& \\sum_{i=1}^n w_ix_i\\\\\n",
        "z &=& f(y)\n",
        "\\end{eqnarray}\n",
        "$$\n",
        "\n",
        "$$\n",
        "f(y) =\n",
        "\\begin{cases}\n",
        "    1\\ (x > 0) \\\\\n",
        "    0\\ (x \\le 0)\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "$w$は重みパラメータの配列、$x$は入力値の配列。\n",
        "計算された値を活性化関数$f$の入力値に使用する。\n",
        "結果、パーセプトロンの出力値は0, 1になる。\n",
        "\n",
        "このパーセプトロンを複数つなげるが、ここで使用する学習方法に誤差逆伝搬法(バックプロパゲーション)がある。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THiOERz9Or4Y",
        "colab_type": "text"
      },
      "source": [
        "## バックプロパゲーションとは\n",
        "参考: https://qiita.com/43x2/items/50b55623c890564f1893\n",
        "\n",
        "誤差逆伝搬法は連鎖律と再急降下法という二つのアルゴリズムの組み合わせ。\n",
        "\n",
        "## 連鎖律 (chain rule)\n",
        "\n",
        "連鎖律とは、複数の関数が組み合わさった合成関数を微分するときのルール。\n",
        "\n",
        "関数$f$と$g_{(n)} (n\\subseteq 1,2,\\cdots,N)$を使って、$z=f(y_1,y_2,\\cdots,y_N), y_n=g_{(n)}(x_1,x_2,\\cdots,x_i,\\cdots)$と表すことができ、以下二つを満たす場合、\n",
        "\n",
        "*   関数$f$が$y_1,\\cdots,y_N$において全微分可能\n",
        "*   関数$g_{(n)}$がそれぞれの$x_i$について偏微分可能\n",
        "\n",
        "以下の式が成り立つ。\n",
        "\n",
        "$$\n",
        "\\begin{eqnarray}\n",
        "\\frac{\\partial z}{\\partial x_i} &=& \\frac{\\partial z}{\\partial y_i}\\cdot \\frac{\\partial y_1}{\\partial x_i} + \\cdots + \\frac{\\partial z}{\\partial y_N}\\cdot \\frac{\\partial y_N}{\\partial x_i}\\\\\n",
        "&=& \\sum_{n=1}^N\\frac{\\partial z}{\\partial y_n}\\cdot \\frac{\\partial y_n}{\\partial x_i}\n",
        "\\end{eqnarray}\n",
        "$$\n",
        "\n",
        "(「合成関数の微分」という名前で数3で習う)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3BxecgNgN4n",
        "colab_type": "text"
      },
      "source": [
        "## 最急降下法\n",
        "\n",
        "微分を使って関数fの傾きが0になる場所を求める方法。重回帰分析でも使った。\n",
        "\n",
        "途中式は省略し、パラメータを求める式を以下に示す。ただし、$w$はパラメータのベクトル、$X$は入力値の行列、$t$は実際のデータの値のベクトル。\n",
        "\n",
        "$$\n",
        "w = (X^TX)^{-1}X^Tt\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iuebiZ42Q58T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}